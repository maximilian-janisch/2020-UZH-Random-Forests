{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning: Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors: Maximilian Janisch and Marco Bertenghi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have been assigned a multi-class classification problem, that is we are interested in correctly predicting if a given input in $\\mathcal{X} = \\mathbb{R}^d$ belongs to one of __three__ classes (i.e. $\\mathcal{Y} = \\{0,1,2\\}$). We are instructed to solve this multi-class classification problem by reducing it to a sequence of binary classification problems (i.e. where $\\mathcal{Y}'=\\{0,1\\}$). We are __not__ allowed to use the multi-class implementation of sklearn and one of the models we must implement must be the random forest model. The goal of the project is then to describe and document in detail how to find and implement the best machine learning algorithm for the dataset assigned to us.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are given a __E-SCOOTER DATASET__. The dataset contains data about the profitability of e-scooter companies. The goal is to predict, as well as possible, the profitability of e-scooter companies by using the information provided. The dataset comes as a csv file (comma-separated values file), named dataset20.csv and contains the following description:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__E-SCOOTER DATASET__\n",
    "\n",
    "----------------------------------------------\n",
    "\n",
    "__Dataset description__:\n",
    "The data is compiled from cities, towns, and small villages in Germany, Austria, and Switzerland, classified according to the profitability of e-scooter companies active in that location. There are __thirteen different features__ associated with each location. The goal is to predict the profitability of an e-scooter company (feature \"class\") from the other features.\n",
    "\n",
    "__Attention!__\n",
    "Please notice that the data has been artificially generated. The dataset does not reflect real-world statistical correlations between features and labels.\n",
    "\n",
    "\tNumber of samples: 500\n",
    "\tNumber of features: 13 (numeric and strings) + one column of class labels (0,1,2)\n",
    "\tFeatures description:\n",
    "\t\tpub_trans: public transport index\n",
    "\t\tprice: price to rent per km\n",
    "\t\ttemperature: average tempereture during summer\n",
    "\t\tinhabitants: number of inhabitants\n",
    "\t\tregistered: number of registered users in thousand\n",
    "\t\tcountry: country\n",
    "\t\tid: internal dataset code\n",
    "\t\tnr_counterparts: nr_counterparts\n",
    "\t\tcars: number of cars per inhabitant\n",
    "\t\tlabour_cost: average labour cost in thousand per month\n",
    "\t\thumidity: absolute humidity in g/m3\n",
    "\t\twindspeed: average wind speed in km/h\n",
    "\t\tsize: size of city center in km2\n",
    "\t\tclass: profitability (0 = loss, 1 = balanced, 2 = profit) <--- LABEL TO PREDICT\n",
    "\n",
    "----------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that:\n",
    "\n",
    "+ We have a total of 500 samples.\n",
    "+ We have a total of 13 features.\n",
    " + Not all features will be relevant, more on that later.\n",
    "+ The profitability is a class, it is the label we want to predict.\n",
    "+ The CSV file uses ';' as a delimiter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# Preliminary code, libraries we want to load\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder  # Todo: der OneHotEncoder wird im Moment nicht verwendet\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis  # im Moment nicht verwendet\n",
    "\n",
    "from RandomForestImplementation.randomForest import RandomForestClassifier\n",
    "from RandomForestImplementation.multiclass import OneVsOneClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Preliminary code, global variables\n",
    "DATA_FILE = os.path.join(\"data\", \"dataset20.csv\")\n",
    "CLASSES = [\"0 = loss\", \"1 = balanced\", \"2 = profit\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read in the data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "with open(DATA_FILE, 'r') as f:\n",
    "    data = csv.reader(f, delimiter = \";\")\n",
    "\n",
    "    row = data.__next__()\n",
    "    rows = [row for row in data]\n",
    "    features_names = np.array(row)\n",
    "    CATEGORICAL_COLUMNS = [5, 6, 7]  # these are the columns \"country\", \"id\" and \"nr_counterparts\" which don't contain numerical data\n",
    "    \n",
    "    x = np.array([row[0:13] for row in rows])  # we put all the features in the x vector\n",
    "    y = np.array([row[13] for row in rows]).astype(int)  # the labels go to the y vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will convert the categorical values to ordinals. This is a bit dangerous because the ordinals have a natural ordering which provides incorrect information about our classifications. However, it doesn't make a difference for the plots or the f-tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.where(x == \"\", np.nan, x)  # This replaces all missing data (imported as \"\") by numpy NaNs. We will get back to missing data during data preprocessing\n",
    "ordinalEnc = OrdinalEncoder()\n",
    "x[:, CATEGORICAL_COLUMNS] = ordinalEnc.fit_transform(x[:, CATEGORICAL_COLUMNS])  # ordinal encoding of the categorical columns\n",
    "x = x.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As a brief sanity check: We can have a look at the feature names and the first representative in x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have the following features: \n",
      " ['pub_trans' 'price' 'temperature' 'inhabitants' 'registered' 'country'\n",
      " 'id' 'nr_counterparts' 'cars' 'labour_cost' 'humidity' 'windspeed' 'size'\n",
      " 'class']\n",
      "--------------------------------\n",
      "The first row looks as follows: \n",
      " [1.42026351e+01 1.90186463e-02 2.51751307e+01 2.35528166e+00\n",
      " 1.18021208e+02 1.00000000e+00 4.25000000e+02 0.00000000e+00\n",
      " 4.13613819e-01 2.23781367e+00 2.66531063e+00 2.83177019e+00\n",
      " 7.48581880e-01]\n",
      "--------------------------------\n",
      "The first row has class label :\n",
      " 2\n"
     ]
    }
   ],
   "source": [
    "print(\"We have the following features: \\n\", features_names)\n",
    "print(\"--------------------------------\")\n",
    "print(\"The first row looks as follows: \\n\", x[0,:])\n",
    "print(\"--------------------------------\")\n",
    "print(\"The first row has class label :\\n\", y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here is the shape of our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the features: \n",
      " (499, 13)\n",
      "Shape of the labels: \n",
      " (499,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of the features: \\n\", x.shape)  # Sanity check, to see what our data looks like.\n",
    "print(\"Shape of the labels: \\n\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are given a varities of features as we've already seen in the problem description. We first want to investigate which features are _relevant_ for our purposes. One suitable way to do this is with the help of data-visualisation, which allows us to draw conclusions about the relevancy of each and every feature (numeric or not).\n",
    "\n",
    "_Example: The feature 'id' stands for the internal dataset code. We consider this feature to be irrelevant as the success of a company will not depend on what they label their products internally. Similarly, the number of counter parts will not play a role when it comes to the success of a company, in fact the customer will not know how many counter parts their product is made of. Hence we will 'forget' about these features._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall first investigate how _complete_ our data is, i.e. we want to see if there are any 'NaN' (Not a Number) values present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 3 called \"inhabitants\" has 4 NaN value(s)\n",
      "Feature 12 called \"size\" has 3 NaN value(s)\n"
     ]
    }
   ],
   "source": [
    "for i in range(x.shape[1]):\n",
    "    if np.isnan(x[:, i]).any():\n",
    "        print(\"Feature\", i, \"called\", f'\"{features_names[i]}\"', \"has\", np.isnan(x[:,i]).sum(), \"NaN value(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code shows that the features 'inhabitants' and 'size' have NaN values present. We can deal with missing data in the following ways:\n",
    "+ Remove the datapoint (but that is not a good idea here since our data set is already small, hence we don't have the luxury removing even more data from our dataset)\n",
    "+ **Data imputation** techniques, i.e. substitute the NaN with a plausible value: \n",
    "    + the mean over non-NaN data of same feature, \n",
    "    + the mid-point of the range of non-NaN data range for same feature, \n",
    "    + the prediction of a regression problem run on the remaining features to predict missing feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We choose the simplest solution and replace the missing values with the mean value of the feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Fixing NaN values in inhabitants feature (index 3) and size feature (index 12)\n",
    "for column_index in range(x.shape[1]):\n",
    "    x[:, column_index] = np.where(np.isnan(x[:, column_index]), np.nanmean(x[:, column_index]), x[:, column_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall now produce and analyse various scatter plots and use them in order to determine whether a feature is relevant or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "def scatterplot_kth_column(k, colors=[\"red\", \"green\", \"blue\"]):\n",
    "    scatter = plt.scatter(range(1, 500), x[:, k], c=y, cmap=ListedColormap(colors))\n",
    "    plt.ylabel(features_names[k])\n",
    "    plt.legend(handles=scatter.legend_elements()[0], labels=CLASSES)\n",
    "\n",
    "    return scatter\n",
    "\n",
    "scatterplot_kth_column(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We observe a clustering of the data with respect to its classes, which we color-coded by RBG (see legend). We conclude that the _pub_trans_ feature is relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scatterplot_kth_column(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We observe a clustering of the data with respect to its classes, which we color-coded by RBG. We conclude that the _price_ feature is relevant. This also makes intuitive sense, that the price will play a role in the success of the companies sales.\n",
    "\n",
    "_Note: Since the data is artificially generated, of course a negative price makes no sense._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scatterplot_kth_column(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We observe a clustering of the data with respect to its classes, which we color-coded by RBG. We conclude that the _temperature_ feature is relevant. This also makes intuitive sense, that the temperature might determine how often people are outside and hence use an E-Scooter, which translates to a companies success in sales. For example: One might not want to start an E-Scooter company in Siberia.\n",
    "\n",
    "_Note: The data is artificial, the most loss seems to be made at around 20-22.5 degrees Celsius. Fair and rather cold weather (i.e. below 17.5 degrees Celsius) still makes balanced profit._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scatterplot_kth_column(9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We observe a clustering of the data with respect to its classes, which we color-coded by RBG. We conclude that the _labour_cost_ feature is relevant. This also makes intuitive sense, that the labour cost plays a role in the companies success. If the labour costs are low, the company is more likely to make a profit than with high labour costs, this is also reflected in the graph above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 15))\n",
    "\n",
    "for k, feature in enumerate([3, 4, 5, 6, 7, 8, 10, 11, 12]):  # scatterplots for some other features\n",
    "    plt.subplot(3, 3, k+1)\n",
    "    scatterplot_kth_column(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The plots above show a much more _chaotic_ behaviour of the data. There is no apparent relationship between the classes and the features. For now we shall consider them as _rather irrelevant_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We still need to discuss whether the feature 'country' is relevant or not. We shall do this by the means of counting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Austria has: 59 instances of loss 62 instances of balanced and 49 instances of profit\n",
      "Germany has: 54 instances of loss 56 instances of balanced and 57 instances of profit\n",
      "Switzerland has: 55 instances of loss 58 instances of balanced and 49 instances of profit\n"
     ]
    }
   ],
   "source": [
    "def counting(number):\n",
    "    \"\"\"\n",
    "    A naive counting function which helps us determine whether a given country is successful in sales or not. \n",
    "    It does so by counting the instances of loss, balanced and profit classes for each country.\n",
    "    Usage: 0 = Austria, 1 = Germany, 2 = Switzerland.\n",
    "    \"\"\"\n",
    "    \n",
    "    country_name = ordinalEnc.inverse_transform([[number,0,0]])[0,0]  # the two zeros after number are not important, they will give the first id and first number of counterparts\n",
    "    \n",
    "    count = np.bincount(y[x[:, 5] == number])\n",
    "    \n",
    "    print(country_name, \"has:\", count[0], \"instances of loss\", count[1], \"instances of balanced and\", count[2], \"instances of profit\")\n",
    "\n",
    "for i in (0,1,2): \n",
    "    counting(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Our analysis suggest that the country feature is also irrelevant as the instances of loss/balance/profit is evenly spread among the three countries. This makes intuitive sense: If a E-Scooter company is successful in one of the three neighbour countries Austria, Germany, Switzerland then it should also be successful in the other two. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "On top of the visual (statistical) analysis we've carried out so far, we shall incorporate an _f-test_ in order to further see which of the features are relevant or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature name     Score\n",
      "pub_trans        838.7327211156739\n",
      "price            507.97390601771116\n",
      "temperature      741.9405923087922\n",
      "inhabitants      0.3797211844807062\n",
      "registered       0.9311583750278856\n",
      "country          0.043267894972736455\n",
      "id               0.9733206062013445\n",
      "nr_counterparts  0.48818426449611024\n",
      "cars             0.23499639468285957\n",
      "labour_cost      380.0286669426751\n",
      "humidity         2.4577930386538798\n",
      "windspeed        2.883947910173691\n",
      "size             1.2846730121640408\n"
     ]
    }
   ],
   "source": [
    "selector = SelectKBest(score_func=f_classif, k=\"all\")\n",
    "selection = selector.fit(x, y)\n",
    "print(f\"{'Feature name':<17}Score\")\n",
    "for feature_name, score in zip(features_names, selection.scores_):\n",
    "    print(f\"{feature_name:<17}{score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conjunction with our visual statistical analysis the _f-test_ confirms our hypothesis of relevant respectively irrelevant features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Evidently, not all features are relevant and we might as well _'forget'_ about them. We recall our features and mark them as relevant respectively irrelevant according to our observations above:\n",
    "\n",
    "\t\tpub_trans: public transport index  (relevant)\n",
    "\t\tprice: price to rent per km (relevant)\n",
    "\t\ttemperature: average tempereture during summer (relevant)\n",
    "\t\tinhabitants: number of inhabitants (irrelevant)\n",
    "\t\tregistered: number of registered users in thousand (irrelevant)\n",
    "\t\tcountry: country (irrelevant)\n",
    "\t\tid: internal dataset code (irrelevant)\n",
    "\t\tnr_counterparts: nr_counterparts (irrelevant)\n",
    "\t\tcars: number of cars per inhabitant (irrelevant)\n",
    "\t\tlabour_cost: average labour cost in thousand per month (relevant)\n",
    "\t\thumidity: absolute humidity in g/m3 (relevant) [maybe only a slight relevance]\n",
    "\t\twindspeed: average wind speed in km/h (relevant) [maybe only a slight relevance]\n",
    "\t\tsize: size of city center in km2 (irrelevant)\n",
    "\t\tclass: profitability (0 = loss, 1 = balanced, 2 = profit) <--- LABEL TO PREDICT !!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In accordance with our observations, we limit ourselves to the following features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "CHOSEN_FEATURES = [0,1,2,9,10,11]\n",
    "X = x[:, CHOSEN_FEATURES]\n",
    "features_names = features_names[CHOSEN_FEATURES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New features: \n",
      " ['pub_trans' 'price' 'temperature' 'labour_cost' 'humidity' 'windspeed']\n",
      "The first row now looks like this: \n",
      " [1.42026351e+01 1.90186463e-02 2.51751307e+01 2.35528166e+00\n",
      " 1.18021208e+02 1.00000000e+00 4.25000000e+02 0.00000000e+00\n",
      " 4.13613819e-01 2.23781367e+00 2.66531063e+00 2.83177019e+00\n",
      " 7.48581880e-01]\n"
     ]
    }
   ],
   "source": [
    "print(\"New features: \\n\", features_names)\n",
    "print(\"The first row now looks like this: \\n\", x[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Shuffling and data train/test splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that usually datapoints in a sample are not realizations of a perfectly _i.i.d. sequence of random variables_. Instead, the sample might be generated in such a way that the datapoints have some degree of time dependence without us noticing, or they might be correlated in some other way among each other. Generally speaking, we might not want to _trust_ the data as it is distributed to us, since we do not know in what way it was gathered (dependencies and so on).\n",
    "\n",
    "Moreover, some algorithms may depend on the order of the datapoints in the sample.\n",
    "\n",
    "In order to reduce these efffects as much as possible,it is always a good idea to __shuffle__ the datapoints before usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, test and validation splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain a result of how good our model will be we cannot plainly use the empirical error obtained on the training dataset. This error is biased as we trained the model in such a way as to minimize it!\n",
    "\n",
    "Instead, one typically would like to compute the empirical error on new, previously __unseen__ data to obtain a better estimate of how good the model describes reality. These \"new, previously unseen\" data __must be set aside from the beginning and separated from the training dataset__ into a test dataset.\n",
    "\n",
    "The test dataset __will not be used for training__ and we will __use it only at the end__, in order to come up with an estimate of how good our model is.\n",
    "\n",
    "The process of evaluating the prediction error of our model using the empirical error on the test dataset is called testing. As a rule of thumb around 20% of your total data should be hold back for testing in the end.\n",
    "\n",
    "Furthermore we want to use some of the training data as a validation set, over which one can evalutate the success of the algorithm’s output predictor. This procedure is called **validation**. In this case, we separate our data set into three distinct sets of labeled examples:\n",
    "\n",
    "1. training set,\n",
    "2. validation set,\n",
    "3. test set.\n",
    "\n",
    "There’s no optimal proportion to split the dataset into these three subsets. The rule of thumb is to use 70% of the dataset for training, 15% for validation and 15% for testing. The validation set is typically used to optimize hyperparameters.\n",
    "\n",
    "Using Scikit learn, the notion of train/test/validation splitting and shuffling can be implemented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The X and y training sets have the shape: \n",
      " (332, 6) (332,)\n",
      "The X and y testing sets have the shape: \n",
      " (83, 6) (83,)\n",
      "The X and y validation sets have the shape: \n",
      " (84, 6) (84,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/3, shuffle=True) # direct shuffle and split with sklearn\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=1/2, shuffle=True)\n",
    "print(\"The X and y training sets have the shape: \\n\", X_train.shape, y_train.shape)\n",
    "print(\"The X and y testing sets have the shape: \\n\", X_test.shape, y_test.shape)\n",
    "print(\"The X and y validation sets have the shape: \\n\", X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "One of the models we must implement (according to the project description) is the random forest model.\n",
    "\n",
    "Our implementation of the RandomForest algorithm already natively supports multiple labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8795180722891566\n"
     ]
    }
   ],
   "source": [
    "forest_multi_classif = RandomForestClassifier(n_estimators=10, random_state=1)\n",
    "forest_multi_classif.fit(X_train, y_train)\n",
    "forest_multi_pred = forest_multi_classif.predict(X_test)\n",
    "print(\"Accuracy:\", (forest_multi_pred==y_test).sum() / len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have also implemented a class *OneVsOneClassifier* which makes it easy to perform OneVsOne classification with any classification estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9036144578313253\n"
     ]
    }
   ],
   "source": [
    "forest_classif = OneVsOneClassifier(RandomForestClassifier, n_classes=3, n_estimators=10, random_state=1)\n",
    "forest_classif.fit(X_train, y_train)\n",
    "forest_pred = forest_classif.predict(X_test)\n",
    "print(\"Accuracy:\", (forest_pred==y_test).sum() / len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Todo: Text anpassen\n",
    "We observe that we get the same percentage of accuracy. The result is also pretty good considering the data is unseen.\n",
    "\n",
    "Next we shall implement some machine learning algorithms provided by scikit-learn. Note that since we are not allowed to use a multiclass classification approach directly (except for the random forest) we must first turn our multiclass classification problem into a binary classification problem. To this extend we make use of the OvO (One vs One) classifier method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\maximilian\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "c:\\users\\maximilian\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8674698795180723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\maximilian\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "MLP_classif = OneVsOneClassifier(MLPClassifier, n_classes=3, learning_rate=\"invscaling\")\n",
    "MLP_classif.fit(X_train, y_train)\n",
    "MLP_pred = MLP_classif.predict(X_test)\n",
    "print(\"Accuracy:\", (MLP_pred==y_test).sum()/len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the exercise classes (class 10) we've seen the $k$-nearnest neighbour algorithm. The $k$-nearest neighbour algorithm is arguably one of the most simple classification algorithm existing. The general idea is to predict the label of a new datapoint according to the label(s) of the closest training point(s) to it. Intuitively, it is a good idea to simply decide the label of a newly observed datapoint according to the label of the closest given point in the training set. Considering only one neighbour would lead to some evident problems. The role of $k$ is that more neighbours means smoother boundary and smoother decision boundary corresponds to simpler model. On the other hand, the boundary for small values of $k$ gets very rough. Hence, the decision rule gets more complicated. Put differently, low k corresponds to high model complexity and high $k$ corresponds to low model complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.891566265060241\n"
     ]
    }
   ],
   "source": [
    "neighbors_classif = OneVsOneClassifier(KNeighborsClassifier, n_classes=3, random_state=1)\n",
    "neighbors_classif.fit(X_train, y_train)\n",
    "neighbors_pred = neighbors_classif.predict(X_test)\n",
    "print(\"Accuracy:\", (neighbors_pred==y_test).sum() / len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Another algorithm we have seen in the exercise classes (class 8) is the support vector machine (SVM)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8795180722891566\n"
     ]
    }
   ],
   "source": [
    "svc_classif = SVC()  # Note that the support vector machine is by default a OneVsOne algorithm\n",
    "svc_classif.fit(X_train, y_train)\n",
    "svc_pred = svc_classif.predict(X_test)\n",
    "print(\"Accuracy:\", (svc_pred==y_test).sum() / len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8795180722891566\n"
     ]
    }
   ],
   "source": [
    "adaboost_classif = OneVsOneClassifier(AdaBoostClassifier, n_classes=3, random_state=1)\n",
    "adaboost_classif.fit(X_train, y_train)\n",
    "adaboost_pred = adaboost_classif.predict(X_test)\n",
    "print(\"Accuracy:\", (adaboost_pred==y_test).sum()/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8795180722891566\n"
     ]
    }
   ],
   "source": [
    "bagging_classif = OneVsOneClassifier(BaggingClassifier, n_classes=3, random_state=1)\n",
    "bagging_classif.fit(X_train, y_train)\n",
    "bagging_pred = bagging_classif.predict(X_test)\n",
    "print(\"Accuracy:\", (bagging_pred==y_test).sum()/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6265060240963856\n"
     ]
    }
   ],
   "source": [
    "gradient_classif = SGDClassifier(random_state=1)  # note that the SGDClassifier is by default a OneVsRest algorithm\n",
    "gradient_classif.fit(X_train, y_train)\n",
    "gradient_pred = gradient_classif.predict(X_test)\n",
    "print(\"Accuracy:\", (gradient_pred==y_test).sum()/len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.891566265060241\n"
     ]
    }
   ],
   "source": [
    "gaussian_classif = OneVsOneClassifier(GaussianProcessClassifier, n_classes=3, random_state=1)\n",
    "gaussian_classif.fit(X_train, y_train)\n",
    "gaussian_pred = gaussian_classif.predict(X_test)\n",
    "print(\"Accuracy:\", (gaussian_pred==y_test).sum()/len(y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}